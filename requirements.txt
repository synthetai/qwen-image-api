# Web framework and server
fastapi>=0.115.2
uvicorn[standard]>=0.24.0

# AI/ML libraries (excluding PyTorch - install manually)
# Install the latest version of diffusers from GitHub
# pip install git+https://github.com/huggingface/diffusers
git+https://github.com/huggingface/diffusers
# Make sure transformers>=4.51.3 (Supporting Qwen2.5-VL)
transformers>=4.51.3
accelerate>=0.24.0

# API and data handling
pydantic>=2.9.2
requests>=2.31.0
Pillow>=9.5.0
starlette>=0.40.0
safetensors>=0.4.0

# NOTE: PyTorch should be installed manually to ensure correct CUDA version
# See installation instructions in README.md

# NOTE: If you encounter flash_attn issues, you can try:
# 1. Uninstall flash_attn: pip uninstall flash-attn
# 2. Or install compatible version: pip install flash-attn --no-build-isolation
# The code automatically disables flash attention if there are compatibility issues
