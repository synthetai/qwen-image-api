# æœåŠ¡å™¨éƒ¨ç½²æŒ‡å—

## ğŸš¨ Flash Attention é—®é¢˜è§£å†³

æ ¹æ®æ‚¨é‡åˆ°çš„é”™è¯¯ï¼Œè¯·åœ¨æœåŠ¡å™¨ä¸ŠæŒ‰ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š

### æ­¥éª¤ 1ï¼šæ¸…ç† Flash Attention æ®‹ç•™æ–‡ä»¶

```bash
# å¸è½½ flash-attn
pip uninstall flash-attn -y

# æ‰‹åŠ¨åˆ é™¤æ®‹ç•™æ–‡ä»¶
sudo rm -rf /usr/local/lib/python3.10/dist-packages/*flash_attn*
sudo rm -rf /usr/local/lib/python3.10/dist-packages/flash_attn*

# æ¸…ç† pip ç¼“å­˜
pip cache purge
```

### æ­¥éª¤ 2ï¼šé‡æ–°å®‰è£… diffusers

```bash
# å¸è½½å¹¶é‡æ–°å®‰è£… diffusers
pip uninstall diffusers -y
pip install git+https://github.com/huggingface/diffusers
```

### æ­¥éª¤ 3ï¼šå¯åŠ¨æœåŠ¡

```bash
# ç›´æ¥å¯åŠ¨ï¼ˆå·²é›†æˆç¯å¢ƒé…ç½®ï¼‰
python3 main.py

# æˆ–è€…æ‰‹åŠ¨è®¾ç½®ç¯å¢ƒå˜é‡å¯åŠ¨
export DIFFUSERS_DISABLE_FLASH_ATTENTION=1
export TOKENIZERS_PARALLELISM=false
python3 main.py
```

## ğŸ” é—®é¢˜è¯Šæ–­

å¦‚æœä»æœ‰é—®é¢˜ï¼Œå¯ä»¥è¿è¡Œï¼š

```bash
# æ£€æŸ¥ç¯å¢ƒ
python3 check_environment.py

# æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯
python3 -c "
import os
os.environ['DIFFUSERS_DISABLE_FLASH_ATTENTION'] = '1'
from diffusers import DiffusionPipeline
print('Success!')
"
```

## ğŸ“ å¸¸è§é—®é¢˜

### é—®é¢˜ 1ï¼šæƒé™ä¸å¤Ÿ
```bash
# ä½¿ç”¨ sudo è¿è¡Œæ¸…ç†è„šæœ¬
sudo python3 fix_flash_attn.py
```

### é—®é¢˜ 2ï¼šPyTorch ç‰ˆæœ¬å†²çª
```bash
# é™çº§åˆ°ç¨³å®šç‰ˆæœ¬
pip install torch==2.1.0 torchvision==0.16.0
pip install git+https://github.com/huggingface/diffusers
```

### é—®é¢˜ 3ï¼šCUDA å†…å­˜ä¸è¶³
```bash
# è®¾ç½®å†…å­˜åˆ†é…ç­–ç•¥
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
python3 start_server.py
```

## ğŸš€ éªŒè¯éƒ¨ç½²

æœåŠ¡å¯åŠ¨åï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼éªŒè¯ï¼š

```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:8000/health

# æŸ¥çœ‹ API æ–‡æ¡£
curl http://localhost:8000/docs
```



## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœä»¥ä¸Šæ­¥éª¤ä»æ— æ³•è§£å†³é—®é¢˜ï¼Œè¯·æä¾›ï¼š
1. æ“ä½œç³»ç»Ÿç‰ˆæœ¬
2. Python ç‰ˆæœ¬
3. PyTorch ç‰ˆæœ¬  
4. CUDA ç‰ˆæœ¬
5. å®Œæ•´çš„é”™è¯¯æ—¥å¿—
